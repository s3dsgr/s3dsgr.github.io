

<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="workshop, computer vision, audio processing, computer graphics, visual learning, machine learning">

  <link rel="shortcut icon" href="static/img/site/favicon.png">

  <title>S3DSGR @ICCV25</title>
  <meta name="description" content="Scalable 3D Scene Generation and Geometric Scene Understanding, ICCV 2025 Workshop">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content=" Scalable 3D Scene Generation and Geometric Scene Understanding Workshop"/>
  <meta property="og:url" content="https://s3dsgr.github.io/"/>
  <meta property="og:description" content="Scalable 3D Scene Generation and Geometric Scene Understanding Workshop"/>
  <meta property="og:site_name" content="Scalable 3D Scene Generation and Geometric Scene Understanding Workshop"/>
  <meta property="og:image" content="Scalable 3D Scene Generation and Geometric Scene Understanding Workshop/static/img/site/teaser.jpg"/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Scalable 3D Scene Generation and Geometric Scene Understanding Workshop"/>
  <meta name="twitter:image" content="Scalable 3D Scene Generation and Geometric Scene Understanding Workshop/static/img/site/teaser.jpg">
  <meta name="twitter:url" content="Scalable 3D Scene Generation and Geometric Scene Understanding Workshop"/>
  <meta name="twitter:description" content="Scalable 3D Scene Generation and Geometric Scene Understanding Workshop, ICCV 2025 Workshop"/>

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

  <style>

    .people-pic {
      max-width: 175px;
      max-height: 175px;
      /*width:300px;*/
      /*height:300px;*/
      /*object-fit: cover;*/
      object-fit: scale-down;
      border-radius: 50%;
  }
  </style>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#cfp">Call for papers</a></li>
        <li><a href="#dates">Schedule</a></li>
        <li><a href="#wsp">Poster Presentation</a></li>      
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
         <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Past Workshops <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../index2024.html" target="__blank">ECCV 2024</a></li>
          </ul>
        </li> 
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>2nd Workshop on Scalable 3D Scene Generation and Geometric Scene Understanding</h1></center>
    <center><h2>ICCV 2025 Workshop</h2></center>
    <center><strong></strong> Oct. 20th (2:00pm - 5:50pm), 2025</center>
  </div>
</div>

<hr />


<!-- <br>
  <center>
  <h1 style="color:red"><a href="https://www.youtube.com/watch?v=gyJDGrbLknI">The <b>video recording</b> of this workshop is here!</a></h1>
  </center>
<br> -->

<!-- <div class="alert alert-info" role="alert">
  <b>Join Zoom Meeting  <a href="https://kaust.zoom.us/j/95818223470">here</a>.</b>
</div> -->



<!-- <div class="row" id="teaser">  
    <div>  
    <img src="static/img/site/teaser.jpg" style="width: 100%; height: auto;"/>
  </div>
</div> -->


<!-- <div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/a.png">
</div>

<div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/b.png">
</div>
 -->






<p><br /></p>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
        Large-scale geometric scene understanding is one of the most critical and long-standing research directions in computer vision, 
        with the impactful applications in autonomous driving, and robotics. Recently, there has been a surge of interest in 3D scene generation,
        driven by its wide-ranging applications in the gaming industry, augmented reality (AR), and virtual reality (VR).
        All these have been transforming our lives and enabling significant commercial opportunities. Both academia and industry have been investing heavily in pushing the
        research directions toward more efficiency and handling the large-scale scene.     </p>
    <p>
       The efficiency and quality of the large-scale reconstruction, and generation rely on the 3D representation and priors applied in solving the problem.
       Moreover, different industries such as robotics, autonomous driving and gaming industry have distinct requirements on the quality and efficiency of the obtained 3D scene structures. 
       The proposed workshop will gather top researchers and engineers from both academia and industry to discuss the future key challenges for this.
      </p>
  </div>
</div>

<p><br /></p>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call For Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
      <p>
        <span style="font-weight:500;">Call for papers:</span> We invite papers of up to 8 pages (in ICCV25 format) for work 
        on tasks related to 3D generation, reconstruction, geometric scene understanding. As the paper will be included the ICCV workshop proceedings,
        no dual submission is accepted.
        Paper topics may include but are not limited to:
      </p>
      <ul>
        <li>Scalable large-scale 3D scene generation</li>
        <li>Efficient 3D representation learning for large-scale 3D scene reconstruction </li>
        <li>Learning compositional structure of the 3D Scene, 3D scalable Object-centric learning</li>
        <li>3D Reconstruction and generation for dynamic scene (with humans and/or rigid objects such as cars)</li>
        <li>Online learning for scalable 3D scene reconstruction </li>
        <li>Foundation models for 3D geometric scene understanding</li>
        <li>3D Reconstruction and Generation for AR/VR/Robotics etc </li>
        <li>Datasets for large-scale scene reconstruction and generation with (moving objects)</li>
        <li>Multi-modal 3D scene generation and geometric understanding </li>
      </ul>
      <p>
        <span style="font-weight:500;">Submission:</span> We encourage submissions of up to 8 pages, excluding references and acknowledgements.
        The submission should be in the ICCV format.
        Reviewing will be double-blind.
        Please submit your paper to the following address by the deadline: 
       <a href="https://cmt3.research.microsoft.com/SDSGR2025/">Submission Portal</a>
      </p>
  </div>
</div>                                                                                        

        <!--
<p><br /></p>
<div class="row" id="wsp">
  <div class="col-xs-12">
    <h2>Poster Presentation</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <table>
      <tbody> 
      <tr><td><a href=" ">#1. Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Raja Kumar, Vanshika Vats, University of California Santa Cruz</font></td></tr>
      <tr><td><a href=" ">#2. On Scaling Up 3D Gaussian Splatting Training</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Hexu Zhao, Haoyang Weng, Daohan Lu, Ang Li, Jinyang Li, Aurojit Panda, Saining Xie, New York University</font></td></tr>
      <tr><td><a href=" ">#3. AEPnP: A Less-constrained EPnP Solver for Pose Estimation with Anisotropic Scaling,</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Jiaxin Wei, Stefan Leutenegger, Laurent Kneip, Technical University of Munich, ShanghaiTech University</font></td> </tr>        
      <tr><td><a href=" ">#4. Scalable Indoor Novel-View Synthesis using Drone-Captured 360 Imagery with 3D Gaussian Splatting,</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Yuanbo Chen, Chengyu Zhang, Jason Wang, Xuefan Gao, Avideh Zakhor, University of California, Berkeley</font></td></tr>
      <tr><td><a href="https://sceneteller.github.io/">#5. SceneTeller: Language-to-3D Scene Generation </a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Başak Melis Öcal, Sezer Karaoğlu, Theo Gevers, ECCV2024 </font></td></tr>
      <tr><td><a href="https://nerf-mae.github.io/">#6. NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira,  Rares Ambrus, ECCV2024 </font></td></tr>
      <tr><td><a href="https://yang-l1.github.io/blockfusion/">#7. BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation,</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui,	weizhe Liu,	Hiroyuki Sato, Hongdong Li,	Pan Ji, SIGGRAPH 2024</font></td> </tr>        
    </tbody></table>
  </div>
</div>
        -->
        
<p><br /></p>

<p><br /></p>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper submission deadline</td>
          <td>June 30th, 2025</td>
        </tr>
        <tr>
          <td>Notifications to accepted papers</td>
          <td>July 10th, 2025</td>
        </tr>
        <tr>
          <td>Paper camera ready</td>
          <td>August 10th, 2025</td>
        </tr>
        <tr>
          <td>Workshop date</td>
          <td>Oct 20th, PM, 2025 </td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Welcome</td>
          <td>2:05pm - 2:10pm</td>
        </tr>
        <tr>
          <td>Angel Xuan Chang<br/> <em> TBD</em></td>
          <td>2:10pm - 2:40pm</td>
        </tr>
        <tr>
          <td>Jia Deng <br/><em> TBD</em> </td>
          <td>2:40pm - 3:10pm</td>
        </tr>
        <tr>
          <td> <br/>Gim Hee Lee<em>TBD</em> </td>
          <td>3:10pm - 3:40pm</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>3:40pm - 4:40pm</td>
        </tr>
        <tr>
          <td><br/>Amir Zamir<em>TBD</em></td>
          <td>4:40pm - 5:10pm</td>
        </tr>
        <tr>
          <td>Kristen Grauman<br /><em>TBD</em></td>
          <td>5:10pm - 5:40pm</td>
        </tr>
        <tr>
          <td>Concluding Remarks</td>
          <td>5:40pm - 5:45pm</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>
       

<p><br /></p>

        <div class="row">
  <div class="col-md-2">
    <a href="https://angelxuanchang.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="speaker/angel.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://angelxuanchang.github.io/">Angel Xuan Chang</a></b>  is an Associate Professor at Simon Fraser University. 
      Prior to this, she was a visiting research scientist at Facebook AI Research and a research scientist at Eloquent Labs working on dialogue. She received my Ph.D. in Computer Science from Stanford,
      where she was part of the Natural Language Processing Group and advised by Chris Manning. Her research focuses on connecting language to 3D representations of shapes and scenes and grounding of
      language for embodied agents in indoor environments. She has worked on methods for synthesizing 3D scenes and shapes from natural language, and various datasets for 3D scene understanding.
      In general, she is interested in the semantics of shapes and scenes, the representation and acquisition of common sense knowledge, and reasoning using probabilistic models.
    </p>
    <p>Title: Compositional scene generation. </p>
  </div>
</div>
<p><br /></p>

        <div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.princeton.edu/~jiadeng/"><img class="people-pic" style="float:left;margin-right:50px;" src="speaker/jiadeng.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a></b> is a Professor of Computer Science at Princeton University, where he directs the Princeton Vision & Learning Lab. 
      He received his Ph.D. from Princeton and his B.Eng. from Tsinghua University, both in computer science. 
      His research spans 3D vision, object and action recognition, and automated reasoning, with interests in per-pixel 3D reconstruction of real-world scenes, visual understanding of objects and interactions, and connections between automated theorem proving, NLP, and AutoML. 
      He is a recipient of the Sloan Research Fellowship, the NSF CAREER Award, the ONR Young Investigator Award, the ICCV Marr Prize, the CVPR Test-of-Time Award, and two ECCV Best Paper Awards.
    </p>

    <p> Title: Data-Centric Visual Learning via Procedural Synthetic Scenes </p>
  </div>
</div>
<p><br /></p>
        





        
<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.utexas.edu/~grauman/"><img class="people-pic" style="float:left;margin-right:50px;" src="speaker/kristen.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.utexas.edu/~grauman/">Kristen Grauman</a></b> is a Professor at the University of Texas at Austin and a Research Director in Facebook AI Research (FAIR).  
      Her research in computer vision and machine learning focuses on video, visual recognition, and action for perception or embodied AI.  
      She and her collaborators have been recognized with several Best Paper awards in computer vision, including a 2011 Marr Prize and a 2017 Helmholtz Prize (test of time award). 
      She served for six years as an Associate Editor-in-Chief for the TPAMI and for ten years as an Editorial Board member for IJCV. She also served as a Program Chair of CVPR2015, NeurIPS2018, and ICCV2023.
      
    </p>
  </div>
</div>
<p><br /></p>

<!--<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.utoronto.ca/~fidler/"><img class="people-pic" style="float:left;margin-right:50px;" src="speaker/fidler.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a></b>  is an Associate Professor at University of Toronto, affiliated faculty at the Vector Institute and VP of AI Research at NVIDIA, leading a
      research lab in Toronto. Prior to that, in 2012/2013, Sanja was a Research Assistant Professor at Toyota Technological Institute at Chicago. Sanja’s
      work is in the area of Computer Vision and Machine Learning, specifically
      the intersection of computer vision and graphics, 3D vision, 3D reconstruction and synthesis; and interactive methods for image annotation.
    </p>
  </div>
</div>
<p><br /></p> -->



        

<div class="row">
  <div class="col-md-2">
    <a href="https://www.comp.nus.edu.sg/~leegh/"><img class="people-pic" style="float:left;margin-right:50px;" src="speaker/GimHee.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a></b> is an Associate Professor in the Department of Computer Science at the National University of Singapore (NUS). He received his PhD in Computer Science from ETH Zurich and was previously a researcher at Mitsubishi Electric Research Laboratories (MERL), USA. He has served as Area Chair for leading conferences such as CVPR, ICCV, ECCV, ICLR, and NeurIPS, and has held organizing roles including Program Chair of 3DV 2022, Demo Chair of CVPR 2023, and General Chair of 3DV 2025. 
      He is a recipient of the Singapore NRF Investigatorship (Class of 2024). His research focuses on 3D computer vision and robotics.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://vilab.epfl.ch/zamir//"><img class="people-pic" style="float:left;margin-right:50px;" src="speaker/Amir.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a></b> is an Assistant Professor of computer science at the Swiss Federal Institute of Technology (EPFL).
      His research is in computer vision and machine learning. Before joining EPFL in 2020, he was with UC Berkeley, Stanford, and UCF. He has received paper awards at SIGGRAPH 2022, CVPR 2020, CVPR 2018, CVPR 2016, and the NVIDIA Pioneering Research Award 2018, PAMI Everingham Prize 2022, and ECCV/ECVA Young Researcher Award 2022. His research has been covered by press outlets, such as The New York Times or Forbes. 
      He was the chief scientist of Aurora Solar, a Forbes AI 50 company, from 2015 to 2022 and currently serves as the chief scientist of Duranta Inc.
      
    </p>
    <p> Title: Multimodal Scene Understanding </p>
  </div>
</div>
<p><br /></p>
        


<!--<p><br /></p>
        <div class="row">
  <div class="col-md-2">
    <a href="https://jiajunwu.com///"><img class="people-pic" style="float:left;margin-right:50px;" src="speaker/Jiajun_Wu.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://jiajunwu.com////">Jiajun Wu</a></b> is an Assistant Professor of Computer Science and of Psychology at Stanford University, working on computer vision, machine learning, and computational cognitive science. Before joining Stanford, he was a Visiting Faculty Researcher at Google Research. He received his PhD in Electrical Engineering and Computer Science from the Massachusetts Institute of Technology. Wu's research has been recognized through the Young Investigator Programs (YIP) by ONR and by AFOSR, the NSF CAREER award,
      paper awards and finalists at ICCV, CVPR, SIGGRAPH Asia, CoRL, and IROS, dissertation awards from ACM, AAAI, and MIT, the 2020 Samsung AI Researcher of the Year, 
      and faculty research awards from J.P. Morgan, Samsung, Amazon, and Meta.
    </p>
  </div>
</div>

<p><br /></p>-->

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
    <div class="col-xs-2">
        <a href="http://users.cecs.anu.edu.au/~mliu/">
          <img class="people-pic" src="orgniser/img/miaomiao.jpg" />
        </a>
        <div class="people-name">
          <a href="http://users.cecs.anu.edu.au/~mliu/">Miaomiao Liu</a>
          <h6>Australian National University, Australia</h6>
        </div>
      </div>

  <div class="col-xs-2">
    <a href="https://alvarezlopezjosem.github.io/">
        <img class="people-pic" src="orgniser/img/jose.jpg" />      
    </a>
    <div class="people-name">
      <a href="https://alvarezlopezjosem.github.io/">Jose M. Alvarez</a>
      <h6>NVIDIA, US</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://people.epfl.ch/mathieu.salzmann/">
      <img class="people-pic" src="orgniser/img/mathieu.jpg" />
    </a>
    <div class="people-name">
      <a href="https://people.epfl.ch/mathieu.salzmann/">Mathieu Salzmann</a>
      <h6>EPFL, Swiss Data Science Center (SDSC), Switzerland</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://lingjie0206.github.io//">
      <img class="people-pic" src="speaker/lingjie.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://lingjie0206.github.io//">Lingjie Liu</a>
      <h6>University of Pennsylvania, US</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://users.cecs.anu.edu.au/~hongdong/">
      <img class="people-pic" src="orgniser/img/hongdong.png" />
    </a>
    <div class="people-name">
      <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>
      <h6>Australian National University, Australia</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://users.cecs.anu.edu.au/~hartley//">
      <img class="people-pic" src="orgniser/img/richard.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://users.cecs.anu.edu.au/~hartley//">Richard Hartley</a>
      <h6>Australian National University, & Google, Australia</h6>
    </div>
  </div>

</div>
<p><br /></p>
<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      To contact the organizers please use <b>S3DSGR@gmail.com</b>
    </p>
  </div>
</div>

<!--        <div class="row" id="Acknowledgement">
  <div class="col-xs-12">
    <h2>Acknowledgement</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The Microsoft CMT service was used for managing the peer reviewing process for this conference. This service was provided for free by Microsoft
      and they bore all expenses including costs for Azure cloud services as well as for software development and support.
    </p>
  </div>
</div>-->
        
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format. </p>
     <p> The Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>
